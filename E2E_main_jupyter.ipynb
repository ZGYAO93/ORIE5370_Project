{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLURXayKqv7N"
   },
   "source": [
    "# Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28927,
     "status": "ok",
     "timestamp": 1747239446193,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "eNHgco-cVpuL",
    "outputId": "27e1279b-1c38-4cf3-cb27-282e25c98064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 481,
     "status": "ok",
     "timestamp": 1747239676200,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "lMb8Mf7eYOEQ",
    "outputId": "7795bcbd-c9b4-48c1-8c5b-4be1550824f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/5370_Project_E2E/E2E-DRO\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/5370_Project_E2E/E2E-DRO/\n",
    "try: import alpha_vantage\n",
    "except ImportError: get_ipython().system('pip install alpha_vantage')\n",
    "\n",
    "try: import cvxpylayers\n",
    "except ImportError: get_ipython().system('pip install cvxpylayers')\n",
    "\n",
    "import numpy as np\n",
    "if np.__version__ != '1.25.2':\n",
    "  get_ipython().system('pip install --upgrade numpy==1.25.2')\n",
    "\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if pd.__version__ != '1.5.3':\n",
    "    get_ipython().system('pip install --upgrade pandas==1.5.3')\n",
    "except ImportError: get_ipython().system('pip install pandas==1.5.3')\n",
    "\n",
    "import matplotlib\n",
    "if matplotlib.__version__ != '3.5.1':\n",
    "  get_ipython().system('pip install --upgrade matplotlib==3.5.1')\n",
    "\n",
    "import os\n",
    "# os.kill(os.getpid(), 9)\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')  # Non-interactive backend that doesn't require TeX\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Isvhyk1q8_Y"
   },
   "source": [
    "# Redo DR Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745980665139,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "A-HTK6ZgbpQO",
    "outputId": "951137d4-d3e5-4042-a744-c3f05cb2b4e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/5370_Project_E2E/E2E-DRO\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/5370_Project_E2E/E2E-DRO/\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#%run main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zhe8sCWlTz-"
   },
   "outputs": [],
   "source": [
    "# Distributionally Robust End-to-End Portfolio Construction\n",
    "# Experiment 1 - General\n",
    "####################################################################################################\n",
    "# Import libraries\n",
    "####################################################################################################\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close(\"all\")\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "\n",
    "# Make the code device-agnostic\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Import E2E_DRO functions\n",
    "from e2edro import e2edro as e2e\n",
    "from e2edro import DataLoad as dl\n",
    "from e2edro import BaseModels as bm\n",
    "from e2edro import PlotFunctions as pf\n",
    "\n",
    "# Path to cache the data, models and results\n",
    "cache_path = \"./cache/exp/\"\n",
    "\n",
    "####################################################################################################\n",
    "# Experiments 1-4 (with hisotrical data): Load data\n",
    "####################################################################################################\n",
    "\n",
    "# Data frequency and start/end dates\n",
    "freq = 'weekly'\n",
    "start = '2000-01-01'\n",
    "end = '2021-09-30'\n",
    "\n",
    "# Train, validation and test split percentage\n",
    "split = [0.6, 0.4]\n",
    "\n",
    "# Number of observations per window\n",
    "n_obs = 104\n",
    "\n",
    "# Number of assets\n",
    "n_y = 20\n",
    "\n",
    "# AlphaVantage API Key.\n",
    "# Note: User API keys can be obtained for free from www.alphavantage.co. Users will need a free\n",
    "# academic or paid license to download adjusted closing pricing data from AlphaVantage.\n",
    "AV_key = None\n",
    "\n",
    "# Historical data: Download data (or load cached data)\n",
    "X, Y = dl.AV(start, end, split, freq=freq, n_obs=n_obs, n_y=n_y, use_cache=True,\n",
    "            save_results=False, AV_key=AV_key)\n",
    "\n",
    "# Number of features and assets\n",
    "n_x, n_y = X.data.shape[1], Y.data.shape[1]\n",
    "\n",
    "# Statistical significance analysis of features vs targets\n",
    "stats = dl.statanalysis(X.data, Y.data)\n",
    "\n",
    "####################################################################################################\n",
    "# E2E Learning System Run\n",
    "####################################################################################################\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Initialize parameters\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Performance loss function and performance period 'v+1'\n",
    "perf_loss='sharpe_loss'\n",
    "perf_period = 13\n",
    "\n",
    "# Weight assigned to MSE prediction loss function\n",
    "pred_loss_factor = 0.5\n",
    "\n",
    "# Risk function (default set to variance)\n",
    "prisk = 'p_var'\n",
    "\n",
    "# Robust decision layer to use: hellinger or tv\n",
    "dr_layer = 'hellinger'\n",
    "\n",
    "# List of learning rates to test\n",
    "lr_list = [0.005, 0.0125, 0.02]\n",
    "\n",
    "# List of total no. of epochs to test\n",
    "epoch_list = [30, 40, 50, 60, 80, 100]\n",
    "\n",
    "# For replicability, set the random seed for the numerical experiments\n",
    "set_seed = 1000\n",
    "\n",
    "# Load saved models (default is False)\n",
    "use_cache = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Rf_LX0meli5z"
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------\n",
    "# Run\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "if use_cache:\n",
    "    # Load cached models and backtest results\n",
    "    with open(cache_path+'ew_net.pkl', 'rb') as inp:\n",
    "        ew_net = pickle.load(inp)\n",
    "    with open(cache_path+'po_net.pkl', 'rb') as inp:\n",
    "        po_net = pickle.load(inp)\n",
    "    with open(cache_path+'base_net.pkl', 'rb') as inp:\n",
    "        base_net = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net.pkl', 'rb') as inp:\n",
    "        nom_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net.pkl', 'rb') as inp:\n",
    "        dr_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_po_net.pkl', 'rb') as inp:\n",
    "        dr_po_net = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_delta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_delta = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_gamma.pkl', 'rb') as inp:\n",
    "        nom_net_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma_delta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma_delta = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_theta.pkl', 'rb') as inp:\n",
    "        nom_net_learn_theta = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_theta.pkl', 'rb') as inp:\n",
    "        dr_net_learn_theta = pickle.load(inp)\n",
    "\n",
    "    with open(cache_path+'base_net_ext.pkl', 'rb') as inp:\n",
    "        base_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_ext.pkl', 'rb') as inp:\n",
    "        nom_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_ext.pkl', 'rb') as inp:\n",
    "        dr_net_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_delta_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_delta_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_gamma_ext.pkl', 'rb') as inp:\n",
    "        nom_net_learn_gamma_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_gamma_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_gamma_ext = pickle.load(inp)\n",
    "    with open(cache_path+'nom_net_learn_theta_ext.pkl', 'rb') as inp:\n",
    "        nom_net_learn_theta_ext = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_learn_theta_ext.pkl', 'rb') as inp:\n",
    "        dr_net_learn_theta_ext = pickle.load(inp)\n",
    "\n",
    "    with open(cache_path+'dr_net_tv.pkl', 'rb') as inp:\n",
    "        dr_net_tv = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_delta.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_delta = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_gamma.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_gamma = pickle.load(inp)\n",
    "    with open(cache_path+'dr_net_tv_learn_theta.pkl', 'rb') as inp:\n",
    "        dr_net_tv_learn_theta = pickle.load(inp)\n",
    "else:\n",
    "    # Exp 1: Equal weight portfolio\n",
    "    ew_net = bm.equal_weight(n_x, n_y, n_obs)\n",
    "    ew_net.net_roll_test(X, Y, n_roll=4)\n",
    "    with open(cache_path+'ew_net.pkl', 'wb') as outp:\n",
    "            pickle.dump(ew_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('ew_net run complete')\n",
    "\n",
    "    # Exp 1, 2, 3: Predict-then-optimize system\n",
    "    po_net = bm.pred_then_opt(n_x, n_y, n_obs, set_seed=set_seed, prisk=prisk).double()\n",
    "    po_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'po_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(po_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('po_net run complete')\n",
    "\n",
    "    # Exp 1: Base E2E\n",
    "    base_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=False, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='base_mod', perf_loss=perf_loss,\n",
    "                        perf_period=perf_period, pred_loss_factor=pred_loss_factor).double()\n",
    "    base_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    base_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'base_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(base_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('base_net run complete')\n",
    "\n",
    "    # Exp 1: Nominal E2E\n",
    "    nom_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=True, train_delta=False,\n",
    "                        set_seed=set_seed, opt_layer='nominal', perf_loss=perf_loss,\n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    nom_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    nom_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'nom_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(nom_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('nom_net run complete')\n",
    "\n",
    "    # Exp 1: DR E2E\n",
    "    dr_net = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                        train_pred=True, train_gamma=True, train_delta=True,\n",
    "                        set_seed=set_seed, opt_layer=dr_layer, perf_loss=perf_loss,\n",
    "                        cache_path=cache_path, perf_period=perf_period,\n",
    "                        pred_loss_factor=pred_loss_factor).double()\n",
    "    dr_net.net_cv(X, Y, lr_list, epoch_list)\n",
    "    dr_net.net_roll_test(X, Y)\n",
    "    with open(cache_path+'dr_net.pkl', 'wb') as outp:\n",
    "        pickle.dump(dr_net, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    print('dr_net run complete')\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# Merge objects with their extended-epoch counterparts\n",
    "####################################################################################################\n",
    "if use_cache:\n",
    "    portfolios = [\"base_net\", \"nom_net\", \"dr_net\", \"dr_net_learn_delta\", \"nom_net_learn_gamma\",\n",
    "                \"dr_net_learn_gamma\", \"nom_net_learn_theta\", \"dr_net_learn_theta\"]\n",
    "\n",
    "    for portfolio in portfolios:\n",
    "        cv_combo = pd.concat([eval(portfolio).cv_results, eval(portfolio+'_ext').cv_results],\n",
    "                        ignore_index=True)\n",
    "        eval(portfolio).load_cv_results(cv_combo)\n",
    "        if eval(portfolio).epochs > 50:\n",
    "            exec(portfolio + '=' + portfolio+'_ext')\n",
    "            eval(portfolio).load_cv_results(cv_combo)\n",
    "\n",
    "####################################################################################################\n",
    "# Numerical results\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "# Validation results table\n",
    "dr_net.cv_results = dr_net.cv_results.sort_values(['epochs', 'lr'],\n",
    "                                                  ascending=[True, True]\n",
    "                                                  ).reset_index(drop=True)\n",
    "exp1_validation_table = pd.concat((base_net.cv_results.round(4),\n",
    "                            nom_net.cv_results.val_loss.round(4),\n",
    "                            dr_net.cv_results.val_loss.round(4)), axis=1)\n",
    "exp1_validation_table.set_axis(['eta', 'Epochs', 'Base', 'Nom.', 'DR'],\n",
    "                        axis=1, inplace=True)\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "portfolio_names = [r'EW', r'PO', r'Base', r'Nominal', r'DR']\n",
    "portfolios = [ew_net.portfolio,\n",
    "              po_net.portfolio,\n",
    "              base_net.portfolio,\n",
    "              nom_net.portfolio,\n",
    "              dr_net.portfolio]\n",
    "\n",
    "# Out-of-sample summary statistics table\n",
    "exp1_fin_table = pf.fin_table(portfolios, portfolio_names)\n",
    "\n",
    "# Wealth evolution plot\n",
    "portfolio_colors = [\"dimgray\",\n",
    "                    \"forestgreen\",\n",
    "                    \"goldenrod\",\n",
    "                    \"dodgerblue\",\n",
    "                    \"salmon\"]\n",
    "port_realized_r = pf.wealth_plot(portfolios, portfolio_names, portfolio_colors,\n",
    "                path=cache_path+\"plots/wealth_exp1.png\")\n",
    "port_sharp_r = pf.sr_bar(portfolios, portfolio_names, portfolio_colors,\n",
    "                path=cache_path+\"plots/sr_bar_exp1.png\")\n",
    "\n",
    "# List of initial parameters\n",
    "exp1_param_dict = dict({'po_net':po_net.gamma.item(),\n",
    "                'nom_net':nom_net.gamma_init,\n",
    "                'dr_net':[dr_net.gamma_init, dr_net.delta_init]})\n",
    "\n",
    "# Trained values for each out-of-sample investment period\n",
    "exp1_trained_vals = pd.DataFrame(zip([nom_net.gamma_init]+nom_net.gamma_trained,\n",
    "                                    [dr_net.gamma_init]+dr_net.gamma_trained,\n",
    "                                    [dr_net.delta_init]+dr_net.delta_trained),\n",
    "                                    columns=[r'Nom. gamma',\n",
    "                                             r'DR gamma',\n",
    "                                             r'DR delta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1747244302748,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "H22vB5p5o6BR",
    "outputId": "05dcacd8-0263-46ff-8e2d-38b8c0e02e72"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n# Create visualizer with all 4 portfolios\\nvisualizer = SimplePortfolioVisualizer(\\n    [equal_portfolio, standard_markowitz_portfolio, max_sharpe_portfolio, risk_budget_portfolio],\\n    [\"Equal Weight\", \"Standard Markowitz\", \"Max Sharpe\", \"Risk Budget\"]\\n)\\n\\n# Create the dashboard\\ndashboard = visualizer.create_dashboard(output_file=\"portfolio_comparison.png\")\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import e2edro.PortfolioClasses as pc\n",
    "\n",
    "def non_overlapping_equal_weight_test(X, Y, n_assets=None, portfolio_constructor=None, holding_period=14):\n",
    "    \"\"\"\n",
    "    Tests performance of an equal-weighted portfolio using non-overlapping periods.\n",
    "    Rebalances at the start of each holding period and tracks daily performance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : object\n",
    "        Features object with test attribute\n",
    "    Y : object\n",
    "        Target/returns object with test attribute\n",
    "    n_assets : int, optional\n",
    "        Number of assets in the portfolio. If None, will be inferred from Y shape.\n",
    "    portfolio_constructor : module, optional\n",
    "        Module with backtest functionality. If None, will attempt to import.\n",
    "    holding_period : int, optional\n",
    "        Number of days to hold the portfolio before rebalancing\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    portfolio : object\n",
    "        Portfolio object with weights, returns, and performance metrics\n",
    "    \"\"\"\n",
    "    # Get test data\n",
    "    X_test = X.test\n",
    "    Y_test = Y.test\n",
    "\n",
    "    # Convert to tensors\n",
    "    Y_test_tensor = torch.tensor(Y_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Get test dates\n",
    "    test_dates = Y_test.index\n",
    "\n",
    "    # Calculate how many non-overlapping periods fit in the test set\n",
    "    available_days = len(test_dates)\n",
    "    num_periods = available_days // holding_period\n",
    "\n",
    "    # Total number of days we'll track in our portfolio\n",
    "    total_days = num_periods * holding_period\n",
    "\n",
    "    # Make sure we don't exceed available data\n",
    "    if total_days > available_days:\n",
    "        total_days = available_days\n",
    "\n",
    "    # Get or infer the number of assets\n",
    "    if n_assets is None:\n",
    "        n_assets = Y_test.shape[1]\n",
    "\n",
    "    # Get or import the portfolio construction modul\n",
    "    # Create portfolio object for storing daily results\n",
    "    portfolio = pc.backtest(total_days - 1, n_assets, test_dates[:total_days - 1])\n",
    "\n",
    "    # Arrays to store weights and daily returns\n",
    "    all_weights = np.zeros((total_days - 1, n_assets))\n",
    "    daily_returns = np.zeros(total_days - 1)\n",
    "\n",
    "    print(f\"Testing equal-weighted portfolio with {holding_period}-day rebalancing across {num_periods} periods\")\n",
    "\n",
    "    # Initialize portfolio value series (starting at 1.0)\n",
    "    portfolio_values = np.ones(total_days)\n",
    "\n",
    "    # Equal weights (1/N for each asset)\n",
    "    equal_weights = np.ones(n_assets) / n_assets\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # For each non-overlapping holding period\n",
    "        for period in range(num_periods):\n",
    "            # Calculate the starting index for this period\n",
    "            period_start_idx = period * holding_period\n",
    "            period_end_idx = min(period_start_idx + holding_period, total_days)\n",
    "\n",
    "            print(f\"Processing period {period+1}/{num_periods} (days {period_start_idx}-{period_end_idx-1})\")\n",
    "\n",
    "            try:\n",
    "                # Store weights for each day in the holding period\n",
    "                for t in range(period_start_idx, period_end_idx - 1):  # -1 because we need next day's data\n",
    "                    all_weights[t] = equal_weights\n",
    "\n",
    "                # Calculate daily returns during the holding period\n",
    "                for t in range(period_start_idx, period_end_idx - 1):\n",
    "                    # Calculate daily return from t to t+1\n",
    "                    curr_prices = Y_test_tensor[t]\n",
    "                    next_prices = Y_test_tensor[t+1]\n",
    "                    daily_price_returns = (next_prices - curr_prices) / curr_prices\n",
    "\n",
    "                    # Portfolio daily return\n",
    "                    weights_tensor = torch.tensor(equal_weights,\n",
    "                                                 dtype=daily_price_returns.dtype,\n",
    "                                                 device=daily_price_returns.device)\n",
    "\n",
    "                    daily_return = float(torch.sum(weights_tensor * daily_price_returns))\n",
    "\n",
    "                    # Store daily return\n",
    "                    daily_returns[t] = np.clip(daily_return, -0.25, 0.25)  # Clip extreme values\n",
    "\n",
    "                    # Update portfolio value\n",
    "                    portfolio_values[t+1] = portfolio_values[t] * (1 + daily_return)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at period {period+1}: {e}\")\n",
    "                # Fill in weights and zeros for returns\n",
    "                for t in range(period_start_idx, period_end_idx - 1):\n",
    "                    if t < len(all_weights):\n",
    "                        all_weights[t] = equal_weights\n",
    "                        daily_returns[t] = 0.0\n",
    "\n",
    "    # Store results in portfolio object\n",
    "    portfolio.weights = all_weights\n",
    "    portfolio.rets = daily_returns\n",
    "\n",
    "    # Calculate portfolio statistics\n",
    "    try:\n",
    "        portfolio.stats()\n",
    "        print(f\"\\nPerformance of Equal-Weighted Portfolio with {holding_period}-day rebalancing:\")\n",
    "        print(f\"Mean Daily Return: {portfolio.mu:.4f}\")\n",
    "        print(f\"Daily Volatility: {portfolio.vol:.4f}\")\n",
    "        print(f\"Sharpe Ratio: {portfolio.sharpe:.4f}\")\n",
    "\n",
    "        # Calculate annualized metrics\n",
    "        annual_factor = 252  # Trading days in a year\n",
    "        annual_return = (1 + portfolio.mu)**annual_factor - 1\n",
    "        annual_vol = portfolio.vol * np.sqrt(annual_factor)\n",
    "        annual_sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "\n",
    "        print(f\"\\nAnnualized Metrics:\")\n",
    "        print(f\"Annual Return: {annual_return:.4f}\")\n",
    "        print(f\"Annual Volatility: {annual_vol:.4f}\")\n",
    "        print(f\"Annual Sharpe Ratio: {annual_sharpe:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in stats calculation: {e}\")\n",
    "        # Manual calculation\n",
    "        returns = np.array(daily_returns)\n",
    "        valid_returns = returns[returns != 0]  # Filter out zeros\n",
    "\n",
    "        if len(valid_returns) > 0:\n",
    "            mean_return = np.mean(valid_returns)\n",
    "            std_return = np.std(valid_returns)\n",
    "\n",
    "            print(f\"Mean Daily Return: {mean_return:.4f}\")\n",
    "            print(f\"Daily Volatility: {std_return:.4f}\")\n",
    "\n",
    "            if std_return > 0:\n",
    "                print(f\"Sharpe Ratio: {mean_return/std_return:.4f}\")\n",
    "\n",
    "                # Annualized metrics\n",
    "                annual_factor = 252  # Trading days in a year\n",
    "                annual_return = (1 + mean_return)**annual_factor - 1\n",
    "                annual_vol = std_return * np.sqrt(annual_factor)\n",
    "                annual_sharpe = annual_return / annual_vol if annual_vol > 0 else 0\n",
    "\n",
    "                print(f\"\\nAnnualized Metrics:\")\n",
    "                print(f\"Annual Return: {annual_return:.4f}\")\n",
    "                print(f\"Annual Volatility: {annual_vol:.4f}\")\n",
    "                print(f\"Annual Sharpe Ratio: {annual_sharpe:.4f}\")\n",
    "        else:\n",
    "            print(\"No valid returns to calculate statistics\")\n",
    "\n",
    "    # Store portfolio values for further analysis\n",
    "    portfolio.values = portfolio_values\n",
    "\n",
    "    return portfolio\n",
    "\n",
    "def compare_to_benchmark(portfolios, benchmark_portfolio, names, holding_period=14):\n",
    "    \"\"\"\n",
    "    Compares model portfolios to an equal-weight benchmark\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    portfolios : list\n",
    "        List of portfolio objects from different models\n",
    "    benchmark_portfolio : object\n",
    "        The benchmark portfolio (equal weights)\n",
    "    names : list\n",
    "        Names of the model portfolios\n",
    "    holding_period : int, optional\n",
    "        Holding period in days\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Performance comparison between models and benchmark\n",
    "    \"\"\"\n",
    "    # Get benchmark performance\n",
    "    benchmark_return = benchmark_portfolio.mu\n",
    "    benchmark_vol = benchmark_portfolio.vol\n",
    "    benchmark_sharpe = benchmark_portfolio.sharpe\n",
    "\n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"Portfolio\": [\"Equal Weight Benchmark\"] + names,\n",
    "        \"Mean Return\": [benchmark_return],\n",
    "        \"Volatility\": [benchmark_vol],\n",
    "        \"Sharpe Ratio\": [benchmark_sharpe],\n",
    "        \"Excess Return vs Benchmark\": [0.0],  # Benchmark compared to itself is 0\n",
    "        \"Information Ratio\": [0.0]  # Benchmark compared to itself is 0\n",
    "    }\n",
    "\n",
    "    # Calculate metrics for each portfolio\n",
    "    for i, portfolio in enumerate(portfolios):\n",
    "        # Get portfolio performance\n",
    "        model_return = portfolio.mu\n",
    "        model_vol = portfolio.vol\n",
    "        model_sharpe = portfolio.sharpe\n",
    "\n",
    "        # Calculate excess return vs benchmark\n",
    "        excess_return = model_return - benchmark_return\n",
    "\n",
    "        # Calculate tracking error (standard deviation of excess returns)\n",
    "        # First, we need to calculate excess returns for each period\n",
    "        excess_rets = np.array(portfolio.rets) - np.array(benchmark_portfolio.rets)\n",
    "        tracking_error = np.std(excess_rets)\n",
    "\n",
    "        # Calculate information ratio\n",
    "        information_ratio = excess_return / tracking_error if tracking_error > 0 else 0\n",
    "\n",
    "        # Add to results\n",
    "        results[\"Mean Return\"].append(model_return)\n",
    "        results[\"Volatility\"].append(model_vol)\n",
    "        results[\"Sharpe Ratio\"].append(model_sharpe)\n",
    "        results[\"Excess Return vs Benchmark\"].append(excess_return)\n",
    "        results[\"Information Ratio\"].append(information_ratio)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(f\"{'Portfolio':<25} {'Mean Return':<12} {'Volatility':<12} {'Sharpe':<12} {'Excess Return':<15} {'Info Ratio':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for i in range(len(results[\"Portfolio\"])):\n",
    "        print(f\"{results['Portfolio'][i]:<25} {results['Mean Return'][i]:>8.4f}    {results['Volatility'][i]:>8.4f}    {results['Sharpe Ratio'][i]:>8.4f}    {results['Excess Return vs Benchmark'][i]:>10.4f}      {results['Information Ratio'][i]:>8.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "class SimplePortfolioVisualizer:\n",
    "    \"\"\"\n",
    "    A streamlined visualization tool for comparing multiple portfolio strategies.\n",
    "    Designed for simplicity and easy-to-read comparisons.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, portfolios, names):\n",
    "        \"\"\"\n",
    "        Initialize with portfolio objects.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        portfolios : list\n",
    "            List of portfolio objects with weights, returns, and dates\n",
    "        names : list\n",
    "            List of names for each portfolio strategy\n",
    "        \"\"\"\n",
    "        self.portfolios = portfolios\n",
    "        self.names = names\n",
    "\n",
    "        # Use distinct colors for better differentiation\n",
    "        self.colors = ['#3366CC', '#DC3912', '#FF9900', '#109618', '#990099']\n",
    "\n",
    "        # Set up basic style\n",
    "        plt.style.use('default')\n",
    "        self.set_plot_style()\n",
    "\n",
    "    def asset_price_movement(self, Y_test=None, normalize=True):\n",
    "        \"\"\"\n",
    "        Plot the underlying assets' price movements during the test period.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y_test : pd.DataFrame or similar\n",
    "            Asset price data with dates as index and assets as columns\n",
    "        normalize : bool\n",
    "            Whether to normalize prices to start from 1.0\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "        \"\"\"\n",
    "        if Y_test is None:\n",
    "            raise ValueError(\"Y_test data must be provided to plot asset price movements\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Get asset values\n",
    "        prices = Y_test.values\n",
    "        dates = Y_test.index\n",
    "\n",
    "        # Get asset names if available, otherwise use indices\n",
    "        if hasattr(Y_test, 'columns'):\n",
    "            asset_names = Y_test.columns\n",
    "        else:\n",
    "            asset_names = [f\"Asset {i+1}\" for i in range(prices.shape[1])]\n",
    "\n",
    "        # Normalize prices if requested\n",
    "        if normalize:\n",
    "            prices = prices / prices[0, :] if len(prices) > 0 else prices\n",
    "\n",
    "        # Colormap for assets\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(asset_names)))\n",
    "\n",
    "        # Plot each asset\n",
    "        for i in range(len(asset_names)):\n",
    "            ax.plot(dates, prices[:, i], label=asset_names[i],\n",
    "                   color=colors[i], linewidth=1.5, alpha=0.8)\n",
    "\n",
    "        # Format plot\n",
    "        title = 'Normalized Asset Price Movements' if normalize else 'Asset Price Movements'\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Price' if not normalize else 'Normalized Price')\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        # Add legend with smaller font size if many assets\n",
    "        if len(asset_names) > 10:\n",
    "            ax.legend(loc='upper left', fontsize=8, ncol=2)\n",
    "        else:\n",
    "            ax.legend(loc='best')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def annual_sharpe_ratio(self, risk_free_rate=0.0):\n",
    "        \"\"\"\n",
    "        Create bar chart of Sharpe ratios by year for each portfolio model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        risk_free_rate : float, optional\n",
    "            Risk-free rate (daily/period rate, not annual)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Group by year for each portfolio and calculate metrics\n",
    "        yearly_sharpe = {}\n",
    "\n",
    "        for i, portfolio in enumerate(self.portfolios):\n",
    "            model_name = self.names[i]\n",
    "            yearly_sharpe[model_name] = {}\n",
    "\n",
    "            # Get returns and dates\n",
    "            returns = np.array(portfolio.rets)\n",
    "            dates = np.array(portfolio.dates)\n",
    "\n",
    "            # Extract period returns if 2D array\n",
    "            if len(returns.shape) > 1 and returns.shape[1] > 0:\n",
    "                period_returns = returns[:, 0]\n",
    "            else:\n",
    "                period_returns = returns\n",
    "\n",
    "            # Get years from dates (handle different date types)\n",
    "            years = []\n",
    "            for d in dates:\n",
    "                if isinstance(d, np.datetime64):\n",
    "                    # For numpy.datetime64, convert to string and extract year\n",
    "                    year_str = str(d).split('-')[0]\n",
    "                    years.append(int(year_str))\n",
    "                elif hasattr(d, 'year'):\n",
    "                    # For datetime.datetime or pandas Timestamp\n",
    "                    years.append(d.year)\n",
    "                else:\n",
    "                    # For other formats, try converting to datetime first\n",
    "                    try:\n",
    "                        date_obj = pd.to_datetime(d)\n",
    "                        years.append(date_obj.year)\n",
    "                    except:\n",
    "                        # If all else fails, just use a placeholder\n",
    "                        years.append(0)\n",
    "\n",
    "            years = np.array(years)\n",
    "            unique_years = np.unique(years)\n",
    "\n",
    "            # Calculate Sharpe ratio for each year\n",
    "            for year in unique_years:\n",
    "                if year == 0:  # Skip invalid years\n",
    "                    continue\n",
    "\n",
    "                year_mask = years == year\n",
    "                year_returns = period_returns[year_mask]\n",
    "\n",
    "                # Only calculate if we have enough data\n",
    "                if len(year_returns) > 10:  # Arbitrary threshold\n",
    "                    mean_return = np.mean(year_returns)\n",
    "                    std_return = np.std(year_returns)\n",
    "\n",
    "                    # Sharpe ratio (avoid division by zero)\n",
    "                    sharpe = (mean_return - risk_free_rate) / std_return if std_return > 0 else 0\n",
    "\n",
    "                    yearly_sharpe[model_name][year] = sharpe\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        all_years = sorted(set().union(*[set(d.keys()) for d in yearly_sharpe.values()]))\n",
    "\n",
    "        # Set up bar positions\n",
    "        bar_width = 0.8 / len(self.portfolios)\n",
    "        positions = np.arange(len(all_years))\n",
    "\n",
    "        # Plot bars for each model\n",
    "        for i, model_name in enumerate(self.names):\n",
    "            model_sharpe = [yearly_sharpe[model_name].get(year, np.nan) for year in all_years]\n",
    "            model_positions = positions + i * bar_width - 0.4 + bar_width / 2\n",
    "\n",
    "            ax.bar(model_positions, model_sharpe, width=bar_width,\n",
    "                  color=self.colors[i % len(self.colors)], alpha=0.7,\n",
    "                  label=model_name)\n",
    "\n",
    "        # Format plot\n",
    "        ax.set_title('Annual Sharpe Ratios by Portfolio Model', fontweight='bold')\n",
    "        ax.set_xlabel('Year')\n",
    "        ax.set_ylabel('Sharpe Ratio')\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels(all_years)\n",
    "\n",
    "        # Add zero line for reference\n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "        # Add legend\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "\n",
    "    def set_plot_style(self):\n",
    "        \"\"\"Set clean style for all plots\"\"\"\n",
    "        plt.rcParams['figure.figsize'] = (10, 6)\n",
    "        plt.rcParams['font.size'] = 12\n",
    "        plt.rcParams['axes.labelsize'] = 14\n",
    "        plt.rcParams['axes.titlesize'] = 16\n",
    "        plt.rcParams['xtick.labelsize'] = 12\n",
    "        plt.rcParams['ytick.labelsize'] = 12\n",
    "        plt.rcParams['legend.fontsize'] = 12\n",
    "        plt.rcParams['axes.spines.top'] = False\n",
    "        plt.rcParams['axes.spines.right'] = False\n",
    "        plt.rcParams['axes.grid'] = True\n",
    "        plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "    \n",
    "    def create_dashboard(self, output_file=None):\n",
    "        \"\"\"\n",
    "        Create a simple, easy-to-read dashboard with the essential visualizations.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_file : str, optional\n",
    "            File path to save the dashboard image\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        matplotlib.figure.Figure\n",
    "        \"\"\"\n",
    "        # Create figure with a 2x2 grid\n",
    "        fig = plt.figure(figsize=(16, 14))\n",
    "        gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1])\n",
    "\n",
    "        # 1. Cumulative Returns Plot (top left)\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "        for i, portfolio in enumerate(self.portfolios):\n",
    "            # Check the shape of returns to determine how to handle them\n",
    "            returns = np.array(portfolio.rets)\n",
    "            dates = np.array(portfolio.dates)\n",
    "\n",
    "            # Check if returns is 2D array and handle appropriately\n",
    "            if len(returns.shape) > 1 and returns.shape[1] > 1:\n",
    "                # If returns has multiple columns, second column might be cumulative returns\n",
    "                if returns.shape[1] == 2:\n",
    "                    # Use the cumulative returns directly (second column)\n",
    "                    cum_returns = returns[:, 1]\n",
    "                else:\n",
    "                    # Use the first column as period returns and calculate cumulative\n",
    "                    period_returns = returns[:, 0]\n",
    "                    cum_returns = np.cumprod(1 + period_returns)\n",
    "            else:\n",
    "                # Traditional 1D array of returns\n",
    "                cum_returns = np.cumprod(1 + returns)\n",
    "\n",
    "            ax1.plot(dates, cum_returns,\n",
    "                    color=self.colors[i % len(self.colors)],\n",
    "                    linewidth=2, label=self.names[i])\n",
    "\n",
    "        ax1.set_title('Cumulative Returns (Growth of $1)', fontweight='bold')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Value')\n",
    "        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        ax1.legend(loc='best')\n",
    "\n",
    "        # 2. Risk-Return Scatter (top right)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "        returns_data = []\n",
    "        volatility_data = []\n",
    "        sharpe_ratios = []\n",
    "\n",
    "        for i in range(len(self.portfolios)):\n",
    "            returns = np.array(self.portfolios[i].rets)\n",
    "\n",
    "            # Extract period returns if returns is a 2D array\n",
    "            if len(returns.shape) > 1 and returns.shape[1] > 0:\n",
    "                period_returns = returns[:, 0]  # First column\n",
    "            else:\n",
    "                period_returns = returns\n",
    "\n",
    "            mean_return = np.mean(period_returns) * 100 *14 # return over 14 days\n",
    "            volatility = np.std(period_returns) * 100\n",
    "            sharpe = mean_return / volatility if volatility > 0 else 0\n",
    "\n",
    "            returns_data.append(mean_return)\n",
    "            volatility_data.append(volatility)\n",
    "            sharpe_ratios.append(sharpe)\n",
    "\n",
    "        # Plot each portfolio\n",
    "        for i in range(len(self.portfolios)):\n",
    "            ax2.scatter(volatility_data[i], returns_data[i],\n",
    "                       color=self.colors[i % len(self.colors)],\n",
    "                       s=120, alpha=0.7,\n",
    "                       label=f\"{self.names[i]} (SR={sharpe_ratios[i]:.2f})\")\n",
    "\n",
    "            ax2.annotate(self.names[i],\n",
    "                        xy=(volatility_data[i], returns_data[i]),\n",
    "                        xytext=(7, 0), textcoords=\"offset points\",\n",
    "                        fontsize=12)\n",
    "\n",
    "        ax2.set_title('Risk-Return Analysis', fontweight='bold')\n",
    "        ax2.set_xlabel('Volatility (%)')\n",
    "        ax2.set_ylabel('Return (%)')\n",
    "        #ax2.legend(loc='upper left')\n",
    "\n",
    "        # 3. Drawdown Chart (bottom left)\n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "        for i, portfolio in enumerate(self.portfolios):\n",
    "            returns = np.array(portfolio.rets)\n",
    "            dates = np.array(portfolio.dates)\n",
    "\n",
    "            # Handle returns based on shape\n",
    "            if len(returns.shape) > 1 and returns.shape[1] > 1:\n",
    "                if returns.shape[1] == 2:\n",
    "                    # If we have cumulative returns, use them directly\n",
    "                    cum_returns = returns[:, 1]\n",
    "                else:\n",
    "                    # Otherwise calculate from period returns\n",
    "                    period_returns = returns[:, 0]\n",
    "                    cum_returns = np.cumprod(1 + period_returns)\n",
    "            else:\n",
    "                cum_returns = np.cumprod(1 + returns)\n",
    "\n",
    "            # Calculate drawdowns\n",
    "            running_max = np.maximum.accumulate(cum_returns)\n",
    "            drawdowns = (cum_returns / running_max - 1) * 100\n",
    "\n",
    "            ax3.plot(dates, drawdowns,\n",
    "                    color=self.colors[i % len(self.colors)],\n",
    "                    linewidth=2, label=self.names[i])\n",
    "\n",
    "            # Add max drawdown point\n",
    "            max_drawdown = np.min(drawdowns)\n",
    "            max_dd_idx = np.argmin(drawdowns)\n",
    "\n",
    "            if max_dd_idx < len(dates):\n",
    "                ax3.scatter(dates[max_dd_idx], drawdowns[max_dd_idx],\n",
    "                           color=self.colors[i % len(self.colors)], s=80, zorder=5)\n",
    "\n",
    "                ax3.annotate(f\"{max_drawdown:.1f}%\",\n",
    "                            xy=(dates[max_dd_idx], drawdowns[max_dd_idx]),\n",
    "                            xytext=(7, -10), textcoords=\"offset points\",\n",
    "                            color=self.colors[i % len(self.colors)], fontsize=10)\n",
    "\n",
    "        ax3.set_title('Portfolio Drawdowns', fontweight='bold')\n",
    "        ax3.set_xlabel('Date')\n",
    "        ax3.set_ylabel('Drawdown (%)')\n",
    "        ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        ax3.legend(loc='lower right')\n",
    "\n",
    "        # 4. Performance Table (bottom right)\n",
    "        ax4 = fig.add_subplot(gs[1, 1])\n",
    "        ax4.axis('off')\n",
    "\n",
    "        # Calculate performance metrics\n",
    "        metrics = []\n",
    "        metric_names = [\n",
    "            'Mean Return (%)',\n",
    "            'Volatility (%)',\n",
    "            'Sharpe Ratio',\n",
    "            'Max Drawdown (%)',\n",
    "            'Win Rate (%)'\n",
    "        ]\n",
    "\n",
    "        for i in range(len(self.portfolios)):\n",
    "\n",
    "            returns = np.array(self.portfolios[i].rets)\n",
    "            dates = np.array(self.portfolios[i].dates)\n",
    "            # Handle returns based on shape\n",
    "            if len(returns.shape) > 1 and returns.shape[1] > 1:\n",
    "                if returns.shape[1] == 2:\n",
    "                    # If we have cumulative returns, use them directly\n",
    "                    cum_returns = returns[:, 1]\n",
    "                else:\n",
    "                    # Otherwise calculate from period returns\n",
    "                    period_returns = returns[:, 0]\n",
    "                    cum_returns = np.cumprod(1 + period_returns)\n",
    "            else:\n",
    "                cum_returns = np.cumprod(1 + returns)\n",
    "\n",
    "            # Example cum_returns (cumulative returns)\n",
    "            cum_returns = np.array(cum_returns)\n",
    "\n",
    "\n",
    "            # Periodic returns from cumulative returns\n",
    "            returns = np.diff(cum_returns) / cum_returns[:-1]\n",
    "\n",
    "            # Final return\n",
    "            final_return = np.mean(returns)*100 * 14  # return over 14 days\n",
    "\n",
    "            # Volatility (standard deviation of returns)\n",
    "            volatility = np.std(returns, ddof=1)*100\n",
    "\n",
    "            # Sharpe ratio (assuming risk-free rate is 0 for simplicity)\n",
    "            sharpe = final_return / volatility if volatility != 0 else np.nan\n",
    "\n",
    "            # Max drawdown\n",
    "            running_max = np.maximum.accumulate(cum_returns)\n",
    "            drawdowns = (cum_returns - running_max) / running_max\n",
    "            max_drawdown = drawdowns.min()\n",
    "\n",
    "            # Win rate (percentage of positive returns)\n",
    "            win_rate = np.mean(returns > 0)\n",
    "            metrics.append([\n",
    "                final_return,  \n",
    "                volatility,   \n",
    "                sharpe,\n",
    "                max_drawdown,\n",
    "                win_rate\n",
    "            ])\n",
    "\n",
    "        # Create table data\n",
    "        cell_text = []\n",
    "        for i, metric_values in enumerate(metrics):\n",
    "            row = [f\"{value:.2f}\" for value in metric_values]\n",
    "            cell_text.append(row)\n",
    "\n",
    "        cell_text = np.array(cell_text).T\n",
    "\n",
    "        # Create table\n",
    "        table = ax4.table(\n",
    "            cellText=cell_text,\n",
    "            rowLabels=metric_names,\n",
    "            colLabels=self.names,\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "\n",
    "        # Style table\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(12)\n",
    "        table.scale(1, 1.5)\n",
    "\n",
    "        # Style headers\n",
    "        for i, name in enumerate(self.names):\n",
    "            cell = table[(0, i)]\n",
    "            cell.set_text_props(weight='bold', color='white')\n",
    "            cell.set_facecolor(self.colors[i % len(self.colors)])\n",
    "\n",
    "        # Style row labels\n",
    "        for i, name in enumerate(metric_names):\n",
    "            cell = table[(i+1, -1)]\n",
    "            cell.set_text_props(weight='bold')\n",
    "            cell.set_facecolor('#f0f0f0')\n",
    "\n",
    "        ax4.set_title('Performance Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "        # Add main title\n",
    "        plt.suptitle('Portfolio Performance Comparison', fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "        if output_file:\n",
    "            plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1747241149031,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "P10iIIBVpVSF",
    "outputId": "0f8f6488-9189-4151-eb2f-450c8ec0ce64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/5370_Project_E2E/E2E-DRO\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/5370_Project_E2E/E2E-DRO/\n",
    "\n",
    "# Distributionally Robust End-to-End Portfolio Construction\n",
    "# Experiment 1 - General\n",
    "####################################################################################################\n",
    "# Import libraries\n",
    "####################################################################################################\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.close(\"all\")\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "\n",
    "# Make the code device-agnostic\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Import E2E_DRO functions\n",
    "from e2edro import e2edro_Markov as e2e\n",
    "from e2edro import DataLoad as dl\n",
    "from e2edro import BaseModels as bm\n",
    "from e2edro import PlotFunctions as pf\n",
    "\n",
    "# Path to cache the data, models and results\n",
    "cache_path = \"./cache/exp_final/\"\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "# Initialize parameters\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "perf_loss='sharpe_loss'\n",
    "perf_period = 13\n",
    "pred_loss_factor = 0.5\n",
    "prisk = 'p_var'\n",
    "dr_layer = 'hellinger'\n",
    "lr_list = [0.0125]# [0.005, 0.0125, 0.02]\n",
    "epoch_list = [40]#[30, 40, 50, 60, 80, 100]\n",
    "set_seed = 1000\n",
    "use_cache = False\n",
    "\n",
    "# ---------------  DATA Collection For Price Predict -----------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import time\n",
    "\n",
    "def fetch_data(tickers, start, end, freq='1d', use_cache=False, save_results=False, cache_path='./cache/original_close.pkl'):\n",
    "    if use_cache:\n",
    "        data = pd.read_pickle(cache_path)\n",
    "    else:\n",
    "        data = pd.DataFrame()\n",
    "        batch_size = 5\n",
    "\n",
    "        for i in range(0, len(tickers), batch_size):\n",
    "            batch = tickers[i:i+batch_size]\n",
    "            try:\n",
    "                print(f\"Downloading batch: {batch}\")\n",
    "                batch_data = yf.download(batch, start=start, end=end, interval=freq, progress=False, group_by='ticker')\n",
    "\n",
    "                # Handle different format if downloading multiple or single tickers\n",
    "                if len(batch) == 1:\n",
    "                    ticker = batch[0]\n",
    "                    batch_data = batch_data['Close'].to_frame(name=ticker)\n",
    "                else:\n",
    "                    batch_data = {ticker: batch_data[ticker]['Close'] for ticker in batch if ticker in batch_data.columns.levels[0]}\n",
    "                    batch_data = pd.DataFrame(batch_data)\n",
    "\n",
    "                data = pd.concat([data, batch_data], axis=1)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {batch}: {e}\")\n",
    "            time.sleep(5)  # pause to avoid rate limiting\n",
    "\n",
    "        if save_results:\n",
    "            data.to_pickle(cache_path)\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_features(data,tickers,lag=5,windows=[10,20,30]):\n",
    "    ret = data.pct_change()\n",
    "    ticker2feature = {}\n",
    "    for ticker in tickers:\n",
    "        df = pd.DataFrame(index=ret.index)\n",
    "        df['close'] = data[ticker]\n",
    "        df['ret'] = ret[ticker]  # actual ret\n",
    "\n",
    "        for l in range(1,lag+1):\n",
    "            df[f'ret_lag_{l}'] = ret[ticker].shift(l)\n",
    "        for w in windows:\n",
    "            df[f'ret_mean_{w}d'] = ret[ticker].rolling(w).mean()\n",
    "        for w in windows:\n",
    "            df[f'ret_vol_{w}d'] = ret[ticker].rolling(w).std()\n",
    "\n",
    "        ticker2feature[ticker] = df\n",
    "    features = pd.concat(ticker2feature.values(), axis=1, keys=ticker2feature.keys())\n",
    "    return features.dropna()\n",
    "tickers = [\"VTI\",\"IWM\",\"AGG\",\"LQD\",\"MUB\",\"DBC\",\"GLD\"]\n",
    "# tickers = ['AAPL', 'MSFT', 'AMZN', 'C', 'JPM', 'BAC', 'XOM', 'HAL', 'MCD', 'WMT',\n",
    "#         'COST', 'CAT', 'LMT', 'JNJ', 'PFE', 'DIS', 'VZ', 'T', 'ED', 'NEM']\n",
    "freq = '1d'\n",
    "start = '2010-01-01'\n",
    "end = '2021-09-30'\n",
    "# Train, validation and test split percentage\n",
    "split = [0.6, 0.4]\n",
    "\n",
    "# Number of observations per window\n",
    "n_obs = 30\n",
    "\n",
    "# Number of features and assets\n",
    "Y_data = fetch_data(tickers, start, end, freq='1d',use_cache=True, save_results=False)\n",
    "X_data = create_features(Y_data, tickers)\n",
    "Y_data = Y_data.iloc[30:]\n",
    "\n",
    "\n",
    "\n",
    "class TimeSeriesDataFrame:\n",
    "    def __init__(self, data, n_obs, split=None):\n",
    "        self.data = data  # Store data as a pandas DataFrame\n",
    "        self.n_obs = n_obs  # Store number of observations for windowing\n",
    "        self.split = split  # Store the train/test split (if provided)\n",
    "\n",
    "        if self.split:\n",
    "            self.train_data, self.test_data = self.train_test_split()\n",
    "\n",
    "        # Set attributes for train and test data directly\n",
    "        self.train = self.train_data\n",
    "        self.test = self.test_data\n",
    "    def train_test_split(self):\n",
    "        split_idx = int(len(self.data) * self.split[0])  # Calculate the index for the split\n",
    "        train_data = self.data.iloc[:split_idx]  # Train data is the first portion\n",
    "        test_data = self.data.iloc[split_idx:]   # Test data is the remaining portion\n",
    "        return train_data, test_data\n",
    "\n",
    "    def split_update(self, split):\n",
    "        self.split = split\n",
    "        self.train_data, self.test_data = self.train_test_split()\n",
    "        self.train = self.train_data\n",
    "        self.test = self.test_data\n",
    "\n",
    "split = [0.6, 0.4]\n",
    "\n",
    "X = TimeSeriesDataFrame(X_data, n_obs=n_obs, split=split)\n",
    "Y = TimeSeriesDataFrame(Y_data, n_obs=n_obs, split=split)\n",
    "n_x, n_y = X.data.shape[1], Y.data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 41104,
     "status": "ok",
     "timestamp": 1747240508631,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "TfAua0zxpbeA",
    "outputId": "02645fe3-08c6-4d6b-c961-0d43e4e79239"
   },
   "outputs": [],
   "source": [
    "# load all the models\n",
    "\n",
    "from e2edro import e2edro_Risk as e2e\n",
    "\n",
    "with open(cache_path+'risk_budget.pkl', 'rb') as inp:\n",
    "        risk_budget = pickle.load(inp)\n",
    "risk_budget_port = risk_budget.non_overlapping_risk_test(X, Y, 14)\n",
    "\n",
    "with open(cache_path+'min_variance.pkl', 'rb') as inp:\n",
    "        min_variance = pickle.load(inp)\n",
    "min_variance_e2e_port = min_variance.non_overlapping_risk_test(X, Y, 14)\n",
    "\n",
    "from e2edro import e2edro_Markov as e2e\n",
    "with open(cache_path+'min_var.pkl', 'rb') as inp:\n",
    "        min_var = pickle.load(inp)\n",
    "min_variance_port = min_var.non_overlapping_test(X, Y, 14)\n",
    "\n",
    "# Create a new model with the same architecture\n",
    "maxSharp_markov_new = e2e.e2e_net(n_x, n_y, n_obs, prisk=prisk,\n",
    "                    train_pred=True, train_gamma=False, train_delta=False,\n",
    "                    set_seed=set_seed, opt_layer='max_sharpe', perf_loss=perf_loss,\n",
    "                    perf_period=perf_period, pred_loss_factor=pred_loss_factor, variant = \"max_sharpe\").double()\n",
    "\n",
    "# Load the saved state - with weights_only=False\n",
    "maxSharp_markov_new.load_state_dict(torch.load(cache_path+'maxSharp_markov_state.pt', weights_only=False))\n",
    "\n",
    "max_sharpe_port = maxSharp_markov_new.non_overlapping_test(X, Y, 14)\n",
    "\n",
    "with open(cache_path+'standard_markov.pkl', 'rb') as inp:\n",
    "        standard_markov = pickle.load(inp)\n",
    "standard_mark_port = standard_markov.non_overlapping_test(X, Y, 14)\n",
    "\n",
    "\n",
    "# Run equal weight test (no model needed)\n",
    "equal_portfolio = non_overlapping_equal_weight_test(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    n_assets=7,  # Specify if known, otherwise it will be inferred from Y\n",
    "    holding_period=14\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2593,
     "status": "ok",
     "timestamp": 1747245712587,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "6oguDcNCpgg9"
   },
   "outputs": [],
   "source": [
    "# Create visualizer with Markovwitze Portfolios\n",
    "visualizer = SimplePortfolioVisualizer(\n",
    "    [equal_portfolio, standard_mark_port, max_sharpe_port],\n",
    "    [\"Equal Weight\", \"Mean Var\", \"Max Shar\"]\n",
    ")\n",
    "\n",
    "# Create the dashboard\n",
    "dashboard = visualizer.create_dashboard(output_file=\"portfolio_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1747240781464,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "JBBEPv_MvkoO"
   },
   "outputs": [],
   "source": [
    "#Plot the asset price movements\n",
    "asset_price_plot = visualizer.asset_price_movement(Y_test=Y.test, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2640,
     "status": "ok",
     "timestamp": 1747244349315,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "qEDT6WbCpi8J"
   },
   "outputs": [],
   "source": [
    "# Create visualizer with Risk portfolios\n",
    "visualizer_risk = SimplePortfolioVisualizer(\n",
    "    [equal_portfolio, min_variance_e2e_port, min_variance_port, risk_budget_port],\n",
    "    [\"Equal Weights\",\"E2E Min Var\", \"Min Var\", \"Risk Budget\"]\n",
    ")\n",
    "\n",
    "# Create the dashboard\n",
    "dashboard_risk = visualizer_risk.create_dashboard(output_file=\"portfolio_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3135,
     "status": "ok",
     "timestamp": 1747244089961,
     "user": {
      "displayName": "Roy Zhou",
      "userId": "03633572487441032120"
     },
     "user_tz": 240
    },
    "id": "bqJbAGKqpvFw"
   },
   "outputs": [],
   "source": [
    "# Create visualizer with all portfolios\n",
    "visualizer_risk = SimplePortfolioVisualizer(\n",
    "    [equal_portfolio, min_variance_e2e_port, min_variance_port, risk_budget_port,standard_mark_port, max_sharpe_port],\n",
    "    [\"EW\",\"e2e Min V\", \"Min V\", \"RB\", \"Mean V\", \"Max Shar\"]\n",
    ")\n",
    "\n",
    "# Create the dashboard\n",
    "dashboard_risk = visualizer_risk.create_dashboard(output_file=\"portfolio_comparison.png\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPqxd7PhhFCAjNWzIV1PmYd",
   "collapsed_sections": [
    "OLURXayKqv7N",
    "8Isvhyk1q8_Y",
    "80MhDbpdqmtj",
    "nz0iocCIKdKj",
    "Lhb-0cnPQEpj"
   ],
   "machine_shape": "hm",
   "mount_file_id": "18LPYzA4cCj_bkYZWT0lHsA3R-UWDCYL-",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
